# ğŸŸ© Projeto â€“ ETL de HistÃ³rico de Vendas para Data Lake

Este projeto demonstra um pipeline de **ETL tradicional** (lote) usando Python e serviÃ§os da AWS. Ele simula um cenÃ¡rio real onde uma empresa precisa processar diariamente seu histÃ³rico de vendas e armazenar os dados em um **Data Lake (Amazon S3)**, permitindo anÃ¡lise posterior com **Amazon Athena**.

---

## ğŸ¯ Objetivo

Extrair dados histÃ³ricos de vendas de um arquivo CSV, realizar transformaÃ§Ãµes simples com Python (pandas) e armazenar os dados no formato Parquet em um bucket Amazon S3, permitindo consultas eficientes com o Amazon Athena.

---

## ğŸ§° Tecnologias e Ferramentas

- ğŸ **Python** (`pandas`, `boto3`, `awswrangler`)
- â˜ï¸ **Amazon S3**
- ğŸ“Š **Amazon Athena**
- ğŸ““ **Google Colab** (ambiente de desenvolvimento)
- ğŸ” **AWS IAM** (configuraÃ§Ã£o de credenciais)

---

## âš™ï¸ Pipeline ETL

1. **ExtraÃ§Ã£o:**
   - Dataset em CSV obtido do Kaggle.
   - Dataset: [EUA Compras Online](https://www.kaggle.com/datasets/zahidmughal2343/usa-online-shopping)

2. **TransformaÃ§Ã£o:**
   - RenomeaÃ§Ã£o e padronizaÃ§Ã£o de colunas.
   - ConversÃ£o de tipos de dados.
   - Limpeza bÃ¡sica.

3. **Carga:**
   - Salvamento dos dados transformados no formato `.parquet`.
   - Upload para o bucket Amazon S3:  
     `s3://data-engineer-projects-jota/projeto1/historico_compras.parquet`

---

## ğŸ” Consultas com Amazon Athena

- Criada tabela no Athena com base nos dados armazenados no S3.
- Consultas SQL realizadas diretamente via `awswrangler` no Google Colab.

---

## ğŸ“Š Resultado Esperado

- âœ” Arquivo `.parquet` salvo no S3 com os dados transformados.
- âœ” Tabela disponÃ­vel no Athena para anÃ¡lise com SQL.
- âœ” Pipeline replicÃ¡vel e escalÃ¡vel para futuras entradas de dados.

---

## ğŸ“š Aprendizados

- âœ… ConexÃ£o segura com AWS usando `boto3` e `awswrangler`
- âœ… Boas prÃ¡ticas de transformaÃ§Ã£o com `pandas`
- âœ… Armazenamento eficiente em Data Lake com Amazon S3
- âœ… AnÃ¡lise de dados usando SQL com Amazon Athena

---

